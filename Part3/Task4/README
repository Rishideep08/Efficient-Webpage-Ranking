In this part of the assignment we will be killing the spark worker process at 25 percent and 75 percent of its lifetime.

To run this algortithm use the following command and run this command from the code file path folder.

{Path to SPark-submit} {code file path} {hdfs file path for input} {hdf file path for output}
example /mnt/data/spark-3.3.0-bin-hadoop3/sbin/spark-submit Ass1_P3task1.py hdfs:10.10.1.1:9000/SortDataInput.csv hdfs:10.10.1.1:9000/SortDataOutput.csv 

or else we can use run.sh script as follows

bash {pathTorun.sh} {Path to Spark-submit} {code file path} {hdfs file path for input} {hdf file path for output}

Example:
 bash ass1-code/Part3/Task4/run.sh /mnt/data/spark-3.3.0-bin-hadoop3/bin/spark-submit ass1-code/Part3/Task4/Ass1_P3task4.py hdfs://10.10.1.1:9000/PageRankData/enwiki-pages-articles/\* hdfs://10.10.1.1:9000/PageRankData/OptiResultsTask4

Note1:
After 2 iterations open any one of the worker node do jps and kill that process.
similarly after 7 iteration kill another worker node. 


we can track the progress from the spark UI.


Note2: 
 if the output folder already exists in hadoop hdfs please delete it before every run.


Note3: Add the below Spark Configuration on spark-defaults.conf file
spark.driver.memory 30g
spark.executor.memory 30g
spark.executor.cores 5
spark.task.cpus 1
spark.local.dir /mnt/data/
spark.speculation true
spark.master spark://10.10.1.1:7077
